import os
import string
from collections import Counter

# generator for the datasets
res_path = '../res/files'
def get_dataset():
  file_list = []
  for path, _, files in os.walk(res_path):
    for name in files:
      file_name = os.path.join(path, name)
      length = os.path.getsize(file_name)
      half_length = int(length / 2)
      file_list.append((file_name, 0, half_length))
      file_list.append((file_name, half_length, length - half_length))
  return file_list


# what to do with the final result
def process_result(result):
  print(result.most_common(5))


# the mapping methods
def map(input):
  # clean the file
  file_name, offset, length = input
  file_contents = read_chunk(file_name, offset, length)
  for char in string.punctuation:
    file_contents = file_contents.replace(char, ' ')
  file_contents = file_contents.lower()

  # get the word counts
  return Counter(file_contents.split())


# The reduce methods
def reduce_start_value():
  return Counter()

def reduce(input, total):
  return input + total


